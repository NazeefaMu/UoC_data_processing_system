## Title: How we help machines  learn
## Supertitle: 
## Section: Features
## Date: Wednesday, February 21, 2018 - 01:00

How we help machines  learn

By age 1, most babies do something that astonishes their parents: They say their first word. It means the child is developing language based on experience. She’s learning.
And while that first word is usually “Mama” or “Dada,” that’s not because the baby wants to say thanks for changing all those diapers (sorry, Mom and Dad). The most frequent first words, which also include “hi,” “baba” (bottle) and “dog,” are those things babies hear most often in their first months of life. Parents label these items and then repeat those labels over and over until the child develops the muscle strength and knowledge to say the words themselves.
In many ways, repetition is how machines learn, too. Engineers train computers to recognize common patterns in mountains of data by exposing them to numerous variations of the same thing. After being “shown” thousands of photos of bank checks, for example, computers learn to recognize both printed and handwritten numbers, which allows us to deposit a check by taking a picture of it using a smartphone. In the same way that babies learn to say “Mama” and reach for their mothers when they want to be held, a computer learns to recognize images of printed numbers so it can identify bank accounts and handwritten numbers to determine a check’s amount.
This process is called machine learning, and it is fundamental to artificial intelligence, or A.I. While A.I. is often seen as mysterious and complicated, in fact the way we teach computers to learn has a lot in common with the way human beings learn. Many A.I. principles are modeled on science’s understanding of how the human brain processes and categorizes information.
Today, A.I. is quietly embedded in our daily lives — many of us don’t even realize it’s there. In addition to simplifying banking, A.I. enables our email to detect spam, our cars to brake automatically and our phones to respond to voice commands. When we interact with digital assistants like Alexa, we’re tapping A.I. Skin cancer can even be diagnosed earlier thanks to A.I.’s learned “memory” of thousands of images of melanoma variations.
Still, we’ve only just begun to scratch the surface of A.I.’s potential. Let’s take a look at how A.I. is being used across industries today and where it may take us in the future.
Trevor Darrell, a professor of computer science at the University of California, Berkeley, explains that while there are core principles that human and machine learning share, there are also key differences.
“The way biology learns is to have situations where thousands or millions of trials are just there for free in the environment,” he says. If shown only a few examples of, say, scissors, “you’ll have no problem telling me what other objects in your environment are that thing. Machines need thousands of labeled examples, both positive and negative.”
One well-developed area of A.I. where this principle has been applied is computer vision, where machines recognize images in a photo (such as the aforementioned check) or a video. Image recognition is what enables Facebook to deliver content its users are interested in: If you’ve liked photos of your friend’s new puppy, Facebook will make sure you don’t miss pics of that puppy in the future.
For visually impaired people, image recognition — plus computer-generated speech — can deliver much more. “We launched a feature based on a computer vision algorithm that allows people who are visually impaired to go through their news feed and understand what photos consist of,” says Joaquin Quiñonero Candela, the head of Facebook’s Applied Machine Learning team, noting that the descriptions are read aloud automatically. “They not only hear that somebody posted a photo, they’re told the photo shows three people in the park riding a bike. That greatly improves the Facebook experience for those users.”
The breakthrough is the result of decades of work. Yann LeCun, chief scientist at Facebook’s Artificial Intelligence Research lab (FAIR) and a founding director of New York University’s Center for Data Science, was among those in the 1980s who recognized that programming a computer to make calculations based on quantifiable outcomes is much different than getting one to make a judgment call that isn’t so straightforward.
“We didn’t know how to write a program that would interpret if an image had a cat, a dog, an airplane or a sailboat in it,” LeCun says. To understand how to solve the problem, you have to think of an image the way a computer does, with each pixel defined by three numbers that correspond to its color and intensity.
“To a computer, an image is just a table of numbers,” LeCun explains. To teach the machine how to interpret that data, LeCun looked to humans. The brain learns based on how strong the links are between neurons — clusters of nerve cells that, when connected, become increasingly associated with a specific idea or physical entity. These are called neural networks. - New York Times
